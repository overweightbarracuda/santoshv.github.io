<!DOCTYPE html>
<link rel="stylesheet" href="style.css">
<html>
<body>
<div>
<h2>Continuous Algorithms: Optimization and Sampling</h2>
<h4>Spring 2020. TR: 9:30-10:45. Arch.(East) 207.</h4>
<h4>Santosh Vempala, OH: Tue 11-12, Klaus 2222.<br>
TA: He Jia, OH: TBD.</h4>

<p>The design of algorithms is traditionally a discrete endeavor. However, many advances have come from a 
continuous viewpoint. Typically, a continuous process, deterministic or randomized is designed (or shown) to have 
desirable properties, such as approaching an optimal solution or a desired distribution, and an algorithm is 
derived from this by appropriate discretization. We will use this viewpoint to develop several general techniques and build up to the state-of-the-art in polynomial algorithms for optimization and sampling.
<ul>
<li>Reduction</li>
<li>Elimination</li>
<li>Conditioning</li>
<li>Geometrization</li>
<li>Expansion</li>
<li>Sparsification</li>
<li>Acceleration</li>
<li>Decomposition</li>
<li>Discretization</li>
</ul>

This will be a more detailed and self-contained version of a <a href="https://santoshv.github.io/contalgos.html">previous edition</a>.<br>
The course is offered simultaneously at UW by <a href="http://yintat.com/teaching/cse535-winter20/">Yin Tat Lee</a>.</p>

<p><b>Prerequisite</b>: basic knowledge of algorithms, probability, linear algebra.</p>
<b><a href="https://www.dropbox.com/s/9ck2p07mibvwrmg/main.pdf">Book</a></b> (in progress)<br></p>

<p><b>Grading</b>:<br>
6550: Biweekly HW, including one longer HW towards the end. You are encouraged to collaborate on HW, but must write your own solutions. Submit via gradescope (link on canvas). <br>
Send comments on lecture notes each week, either in dropbox or by email.<br>
8803: HW optional. Comment on lecture notes each week.<br>
Bonus: suggesting simpler proofs, exercises, figures.
</p>

<b>Schedule</b> (tentative):<br>

<ol>

<li> Introduction<br>

Jan 7. Course overview. <br> 
Jan 9. Gradient descent.<br>
<b>HW1</b> has been posted on gradescope, due Jan 20.<br>
Jan 14. Gradient descent (contd.)

<li>Elimination<br>

Jan 14, 16. Ellipsoid/CoG/Cutting Plane.<br>
Jan 16. Bolas y Parabolas.

<li>Reduction<br>

Jan 21, 23. Duality (LP duality, SDP duality, ...) and Equivalences (Optimization, Membership, Separation; OPT, Integration->Sampling; Maxflow, Bipartite Matching).

<li>Geometrization I<br>

Jan 28. Lower bounds <br>
Jan 30. Mirror Descent <br>
Feb 4. Frank-Wolfe <bR>
Feb 6. Newton method<br>
Feb 11. Interior Point Method<br>
Feb 13. IPM for LP.

<li>Sparsification<br>

Feb 18. Regression and subspace embeddings.<br>
Feb 20. Matrix approximation.<br>
Feb 25. Coordinate Descent.<br>
Feb 27. Stochastic Gradient Descent.

<li> Acceleration<br> 

Mar 3. Conjugate Gradient and Chebychev Expansion.<br>
Mar 5. Accelerated Gradient Descent.

<li> Decomposition<br>

Mar 10. <br>
Mar 12. <br>
Mar 17,19 (Spring break)<br>


<li>Sampling<br>

Mar 24. Langevin Dynamics and SDE.<br>
Mar 26. Conductance, Ball walk<br>

<li>Geometrization II<br>

Apr 7. Mixing of ball walk, Isoperimetry, Isotropy.<br>
Apr 9. Hit-and-Run, Dikin walk<br>
Apr 14. (Riemannian) HMC<br>
Apr 16. ODE. Complex analysis.<br>

</ol>


</p>
</div>
</body>
