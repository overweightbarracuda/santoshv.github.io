<!DOCTYPE html>
<link rel="stylesheet" href="style.css">
<html>
<body>
<div>
<h2>Continuous Algorithms: Optimization and Sampling</h2>
<h4>Spring 2020. TR: 9:30-10:45. Arch.(East) 207.</h4>
<h4>Santosh Vempala, OH: Tue 11-12, Klaus 2222.<br>
TA: He Jia, OH: Thu 2-4, Arthita Ghosh: OH: Mon 11-1.</h4>

<p>The design of algorithms is traditionally a discrete endeavor. However, many advances have come from a 
continuous viewpoint. Typically, a continuous process, deterministic or randomized is designed (or shown) to have 
desirable properties, such as approaching an optimal solution or a desired distribution, and an algorithm is 
derived from this by appropriate discretization. We will use this viewpoint to develop several general techniques and build up to the state-of-the-art in polynomial algorithms for optimization and sampling.
<ul>
<li>Reduction</li>
<li>Elimination</li>
<li>Conditioning</li>
<li>Geometrization</li>
<li>Expansion</li>
<li>Sparsification</li>
<li>Acceleration</li>
<li>Decomposition</li>
<li>Discretization</li>
</ul>

The course is offered simultaneously at UW by <a href="http://yintat.com/teaching/cse535-winter20/">Yin Tat Lee</a>.</p>

<p><b>Prerequisite</b>: basic knowledge of algorithms, probability, linear algebra.</p><br>

<b><a href="https://www.dropbox.com/s/9ck2p07mibvwrmg/main.pdf">Textbook</a></b> (in progress)<br></p><br>

<p><b>Grading</b>:<br>
6550: Biweekly HW, including one longer HW towards the end. You are encouraged to collaborate on HW, but must write your own solutions. Submit via gradescope (link on canvas). <br>
Send comments on lecture notes each week, either in dropbox or by email.<br>
8803: HW optional. Comment on lecture notes each week.<br>
Bonus: suggesting simpler proofs, exercises, figures.<br>
</p>
<br>

<b>Schedule</b> (tentative):
<ol>
<li> Introduction<br>
Jan 7. Course overview. <br> 
Jan 9. Gradient descent. <a href="L1Jan7.pdf">Notes</a>.<br>
<b>HW1</b> has been posted on gradescope, due Jan 20.<br>
Jan 14. Gradient descent (contd.) <a href="L2Jan14.pdf">Notes</a>.

<li>Elimination<br>

Jan 16. Cutting Plane method; Ellipsoid. <a href="L3Jan16.pdf">Notes</a> (and from <a href="https://www.cc.gatech.edu/~vempala/acg/www/18.433/L1617.pdf">a while ago</a>)<br>
Jan 21. Center-of-Gravity. <a href="L4Jan21.pdf">Notes</a><br>
Jan 23. Ball method; lower bounds.

<li>Reduction<br>

Jan 28, 30. Duality (LP duality, SDP duality, ...) and Equivalences (Optimization, Membership, Separation; OPT, Integration->Sampling; Maxflow, Bipartite Matching).

<li>Geometrization I<br>

Feb 4. Mirror Descent, Frank-Wolfe <bR>
Feb 6. Newton method<br>
Feb 11. Interior Point Method<br>
Feb 13. IPM for LP.

<li>Sparsification<br>

Feb 18. Regression and subspace embeddings.<br>
Feb 20. Matrix approximation.<br>
Feb 25. Coordinate Descent.<br>
Feb 27. Stochastic Gradient Descent.

<li> Acceleration<br> 

Mar 3. Conjugate Gradient and Chebychev Expansion.<br>
Mar 5. Accelerated Gradient Descent.

<li> Decomposition<br>

Mar 10. <br>
Mar 12. <br>
Mar 17,19 (Spring break)<br>


<li>Sampling<br>

Mar 24. Langevin Dynamics and SDE.<br>
Mar 26. Conductance, Ball walk<br>

<li>Geometrization II<br>

Apr 7. Mixing of ball walk, Isoperimetry, Isotropy.<br>
Apr 9. Hit-and-Run, Dikin walk<br>
Apr 14. (Riemannian) HMC<br>
Apr 16. ODE. Complex analysis.<br>

</ol>


</p>
</div>
</body>
