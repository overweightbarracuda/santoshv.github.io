<!DOCTYPE html>
<link rel="stylesheet" href="style.css">
<html>
<body>
<div>
<h2>Continuous Algorithms: Optimization and Sampling</h2>
<h4>Spring 2020. TR: 9:30-10:45. Arch.(East) 207.</h4>
<h4>Santosh Vempala, OH: Tue 11-12, Klaus 2222.<br>
TA: He Jia, OH: Thu 2-4, Arthita Ghosh: OH: Mon 11-1.</h4>

<p>The design of algorithms is traditionally a discrete endeavor. However, many advances have come from a 
continuous viewpoint. Typically, a continuous process, deterministic or randomized is designed (or shown) to have 
desirable properties, such as approaching an optimal solution or a desired distribution, and an algorithm is 
derived from this by appropriate discretization. We will use this viewpoint to develop several general techniques and build up to the state-of-the-art in polynomial algorithms for optimization and sampling.
<ul>
<li>Reduction</li>
<li>Elimination</li>
<li>Conditioning</li>
<li>Geometrization</li>
<li>Expansion</li>
<li>Sparsification</li>
<li>Acceleration</li>
<li>Decomposition</li>
<li>Discretization</li>
</ul>

The course is offered simultaneously at UW by <a href="http://yintat.com/teaching/cse535-winter20/">Yin Tat Lee</a>.</p>

<p><b>Prerequisite</b>: Basic knowledge of algorithms, probability, linear algebra.</p><br>

<b><a href="https://github.com/YinTat/optimizationbook/blob/main/main.pdf">Textbook (updated May 2023)</a></b> (in progress)<br></p><br>

<p><b>Grading</b>:<br>
6550: Biweekly HW, including one longer HW towards the end. You are encouraged to collaborate on HW, but must write your own solutions. Submit via gradescope (link on canvas). <br>
Send comments on lecture notes each week, either in dropbox or by email.<br>
8803: HW optional. Comment on lecture notes each week.<br>
Bonus: suggesting simpler proofs, exercises, figures.<br>
</p>
<br>

<b>Schedule</b> (tentative):
<ol>
<li> Introduction<br>
Jan 7. Course overview. <br> 
Jan 9. Gradient descent. <a href="L1Jan7.pdf">Notes</a>.<br>
Jan 14. Gradient descent (contd.) <a href="L2Jan14.pdf">Notes</a>.

<li>Elimination<br>

Jan 16. Cutting Plane method; Ellipsoid. <a href="L3Jan16.pdf">Notes</a> (and from <a href="https://www.cc.gatech.edu/~vempala/acg/www/18.433/L1617.pdf">a while ago</a>)<br>
Jan 21. Center-of-Gravity. <a href="L4Jan21.pdf">Notes</a><br>
Jan 23. Ball method; lower bounds. <a href="L5Jan23.pdf">Notes</a><br>

<li>Reduction<br>

Jan 28. Duality (LP duality, SDP duality, ...). <a href="L6Jan28.pdf">Notes</a><br> 
Jan 30, 4. Equivalences (Optimization, Membership, Separation; Gradient, Evaluation). <a href="L7Jan30.pdf">Notes</a><br>

<li>Geometrization I<br>

Feb 4. Mirror Descent. <a href="L9Feb4.pdf">Notes</a><br>
Feb 6. Frank-Wolfe. <a href="L10Feb6.pdf">Notes</a><br>
Feb 11. Newton method<br>
Feb 13, 18. Interior Point Method for LP.<a href="L13Feb20.pdf">Notes</a><br>
Feb 20, 25. Self-concordance and IPM for convex optimization.<br>

<li>Sparsification<br>

Mar 3. Regression and subspace embeddings.<a href="L15Mar2.pdf">Notes</a><br>
Mar 5. Matrix approximation.<br>

<li> Acceleration<br> 

Mar 10. Linear Regression and Chebychev Polynomials. <a href="L16Mar10.pdf">Notes</a><br>
Mar 12. Conjugate Gradient. <a href="L17Mar12.pdf">Notes</a><br>

Mar 17,19 (Spring break)

<li>Sampling<br>

Mar 24. Langevin Dynamics and SDE.<br>
Mar 26. Conductance and Convergence<br>

<li>Geometrization II<br>

Apr 7. Mixing of the ball walk, Isotropy.<br>
Apr 9. Isoperimetry, KLS<br>
Apr 14. Volume Computation <br>  
Apr 16. Gaussian Cooling <br>

Additional topics: (Riemannian) HMC, ODE, Complex analysis.<br>

</ol>


</p>
</div>
</body>
