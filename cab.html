<!DOCTYPE html>
<link rel="stylesheet" href="style.css">
<html>
<body>
<div>
<h2>Computation and Brain</h2>
<h3>Santosh Vempala</h3>
<h4>Spring 2021. </h4>
<p>
Despite breathtaking advances in Neuroscience, there are no overarching theories of how the brain accomplishes all that it does. Algorithms and computing have also progressed rapidly, often using faster and more powerful hardware, as well as problem-specific and sophisticated methods. Taking the view that the brain must be, at heart, a computational entity, this intensive seminar will begin with a few classic readings in cognition/neuroscience; discuss relevant models of computational complexity; highlight the gaps between experiment, theory and our understanding; and formulate concrete problems and challenges for both neuro/cognitive scientists and computer scientists.

Prior knowledge of neuroscience or complexity is welcome, but will not be assumed.
</p>
<ul>
<li>Reduction</li>
<li>Elimination</li>
<li>Conditioning</li>
<li>Geometrization</li>
<li>Expansion</li>
<li>Sparsification</li>
<li>Acceleration</li>
<li>Decomposition</li>
<li>Discretization</li>
</ul>
The course is also being taught on the West coast by <a href="http://yintat.com/teaching/cse535-winter19/">Yin Tat Lee</a>.</p><br>

<p><b>Prerequisite</b>: basic knowledge of algorithms, probability, linear algebra.</p>
<p><b><a href="https://www.dropbox.com/s/24hm80x5z0rf2wh/main.pdf?dl=0">Lecture notes</a></b>.<br>
<br>
</p>
<p>
  <b>Schedule</b> (tentative):<br>
<ol>
<li> Introduction<br>
Jan 7. Course overview. Gradient descent<br> 
Jan 9. Langevin dynamics

<li>Elimination<br>
Jan 14. Ellipsoid/CoG/Cutting Plane for Optimization<br>
Jan 16. ... for Volume computation<br>
Jan 21. MLK holiday<br>
Jan 23. Bolas y Parabolas

<li>Reduction<br>
Jan 28, 30. Duality (LP duality, SDP duality, ...) and Equivalences (Optimization, Membership, Separation; OPT, Integration->Sampling; Maxflow, Bipartite Matching)

<li>Geometrization I: Optimization<br>
Feb 4. Newton method<br>
Feb 6. IPM for LP<br>
Feb 18. Andre Wibisono: Higher-Order Gradient Methods<br>  

<li>Geometrization II: Sampling<br>
Feb 11. ARC 12.<br>
Feb 13. Conductance, Ball walk<br>
Feb 25. Mixing of ball walk, Isoperimetry, Isotropy.<br>
Feb 27. Hit-and-Run<br> 
Mar 4. Dikin walk<br>
Mar 6. (Riemannian) HMC<br>

<li> Multiplication<br>
Mar 11. Simulated Annealing.<br>
Mar 13. Volume, Integration, Rounding. <br>

Mar 18,20 (Spring break)<br>

<li> Expansion<br>
Mar 25. ODE. Collocation Method. Complex analysis.<br>
Mar 27. SDE. Ito's lemma. Localization.<br>

<li> Sparsification<br>
Apr 1. Regression and subspace embeddings.<br>

<li> Acceleration<br> 
Apr 3. Accelerated Gradient Descent, SGD, SVRG, CG. <br>

<li> Student presentations<br>
Apr 8.<br>
Apr 10.<br>
Apr 15.<br>
Apr 17.<br>
</ol>


<b>Format</b>: This course is a research seminar. I will give most of the lectures and class participants will give the
rest.<br> 
A student taking the class for credit is expected to (possibly partnering with another student): <br>
(a) Each week: Proofread lecture notes and suggest improvements OR do one HW problem.<br>
(b) Present in class <br>
(c) Study in more depth the topic of (a) or (b) and prepare an expository note and/or work on a related open problem.<br>
</p>
</div>
</body>
</html>
