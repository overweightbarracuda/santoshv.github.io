<!DOCTYPE html>
<link rel="stylesheet" href="MLstyle.css">
<html>
<body>
<div>
<h2>Machine Learning Theory, CS7545</h2>
<h3>Fall 2021. MW: 9:30-10:45. Kendeda 210.</h3>
<h3>Instructor: <a href="http://www.cc.gatech.edu/~vempala">Santosh Vempala</a>, OH: Tue 11-12, benches outside Klaus.<br>
TAs: He Jia, OH: Thu 3-5pm; Aditi Laddha, OH: Fri 3-5pm; outside Klaus 2116.</h3>

<p><b>Prerequisite</b>: basic knowledge of algorithms, probability, linear algebra.</p><br>

<b>Grading</b>:<br>
<ul>
<li>4 HW sets (50%). You are encouraged to collaborate on homework (except HW0, which you must do on your own), and expected to write your own solutions. Submit via gradescope (link on canvas). <br>
<li>2 Midterm exams (40%).<br>
<li>1 Research report on one current topic in ML of theoretical interest (10%).<br>
<br>
</ul>

<b>References</b>:<br>
<ul>
<li><a href="https://home.ttic.edu/~avrim/book.pdf">Foundations of Data Science</a> (FoDS), Blum, Hopcroft, Kannan.<br>
<li>An Introduction to Computational Learning Theory, Kearns and U. Vazirani<br>
<li><a href="https://www.cc.gatech.edu/~vempala/spectralbook.pdf">Spectral Algorithms</a>, Kannan, Vempala<br>
<li>Avrim Blum's Learning Theory <a href="https://home.ttic.edu/~avrim/MLT18/index.html">course</a>.<br>
<li>Some <a href="references.pdf">research articles</a>
</ul>

<p><b>Schedule</b> (tentative):
<ol>
<li> Introduction </li>
Aug 23. Topics overview. <a href="overview.pdf">Notes</a>. <a href="hw0.pdf">HW0</a>. <br> 
<br>

<li> Unsupervised Learning</li>

Aug 25 Statistical Estimation: Any single Gaussian. <a href="L8_24.pdf">Notes</a>. <a href="08_25_scribed.pdf">Scribed</a>. Chap 2 of FoDS.<br>
Aug 30, Sep 1. Mixture models, PCA, Random Projection. <a href="L8_30.pdf">Notes</a>. Chapters 1 and 2 <a href="spectral_algorithms.pdf">here</a>. Chap 3 of FoDS.<br> 
Sep 6. Labor day. <br>
Sep 8. Tensor factorization. Chap 7 <a href="spectral_algorithms.pdf">here</a>. <a href="hw1.pdf">HW1</a>. <br>
Sep 13. Independent Component Analysis. <a href="L9_13.pdf">Notes</a>. Chap 3 <a href="spectral_algorithms.pdf">here</a>.<br>
Sep 15. Robust Estimation. <a href="L9_15.pdf">Notes</a>. <a href="L9_15_scribed.pdf">Scribed</a>.<br>
Sep 20, 22, 27. Clustering: k-means, spectral, hierarchical, fair. <br>
<a href="L9_20.pdf">Notes1</a>, <a href="L9_27.pdf">Notes2</a>. Chap 7 of FoDS. Chap 5 <a href="spectral_algorithms.pdf">here</a>.
<br>
Sep 29. Midterm I.<br>
<br>

<li>Supervised Learning</li>

Oct 4. PAC, Mistake-bound models.<br>
Oct 6. Halfspace learning, Perceptron, kernel trick. <a href="L10_6.pdf">Notes</a>. FoDS, Chap 6. <br>
Oct 11. Fall recess. <br>
Oct 13. Experts, Weighted Majority, Winnow. <a href="L10_13.pdf">Notes</a>. <a href="lec10_04+10_06+10_13.pdf">Scribed</a> (10/4-10/13). <a href="hw2.pdf">HW2</a>.<br>
Oct 18, 20. VC-dimension. <a href="L10_18.pdf">Notes</a>. <br>
Oct 25. Boosting. <a href="L10_25.pdf">Notes</a>. <br>
Oct 27. Support Vector Machines. <br> 
Nov 1. Fourier learning <br>
Nov 3. Statistical query model: lower bound for parity</br>
Nov 8. Follow the perturbed leader.<br>
Nov 10. Online convex optimization. <br>
Nov 15. Midterm II<br>
<br>

<li> Contemporary theories of Neural Networks</li>

Nov 17, 22. Deep Neural Networks. <br>
Nov 29, Dec 1. Brain.<br>
Dec 6. Transformers. <br>
 

</ol>
</p>

</p>
</div>
</body>
